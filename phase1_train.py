"""
Stage 1: The Captioner - Main Training Script
Deep-ML Style: Memory-efficient QLoRA fine-tuning for Llama 3.2 Vision.

Philosophy: Mathematically clean, technically robust, production-ready.
"""
import os
import sys
import argparse
import logging
from datetime import datetime
from typing import Optional

import torch
import torch.nn as nn
from torch.cuda.amp import GradScaler
import matplotlib.pyplot as plt
from tqdm import tqdm

from transformers import (
    MllamaForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)

from config import get_config, Phase1Config
from dataset import create_dataloader

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class MultimodalTrainer:
    """
    Deep-ML Style Implementation of Stage 1: The Captioner.
    Focus: Memory-efficient fine-tuning via QLoRA on A100 80GB.
    """
    
    def __init__(self, config: Phase1Config):
        self.config = config
        self.device = config.device
        self.global_step = 0
        self.losses = []
        self.best_loss = float('inf')
        
        logger.info("=" * 60)
        logger.info("Stage 1: The Captioner - Initializing")
        logger.info("=" * 60)
        
        # Set seed for reproducibility
        torch.manual_seed(config.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(config.seed)
        
        # 1. Configure Quantization (NF4 for memory efficiency)
        quantization_config = None
        if config.training.use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True  # Nested quantization
            )
            logger.info("[Model] Using 4-bit NF4 quantization with double quant")
        
        # 2. Load Processor
        logger.info(f"[Model] Loading processor: {config.training.model_id}")
        self.processor = AutoProcessor.from_pretrained(config.training.model_id)
        
        # 3. Load Model
        logger.info(f"[Model] Loading model: {config.training.model_id}")
        self.model = MllamaForConditionalGeneration.from_pretrained(
            config.training.model_id,
            quantization_config=quantization_config,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            attn_implementation="sdpa"  # Use Flash Attention if available
        )
        
        # 4. Prepare for k-bit training
        if config.training.use_4bit:
            self.model = prepare_model_for_kbit_training(
                self.model,
                use_gradient_checkpointing=True
            )
            logger.info("[Model] Prepared for k-bit training with gradient checkpointing")
        
        # 5. Configure LoRA (Mathematical Adapters)
        lora_config = LoraConfig(
            r=config.lora.r,
            lora_alpha=config.lora.lora_alpha,
            target_modules=config.lora.target_modules,
            lora_dropout=config.lora.lora_dropout,
            bias=config.lora.bias,
            task_type=TaskType.CAUSAL_LM
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self._print_trainable_params()
        
        # 6. Setup Optimizer
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=config.training.learning_rate,
            weight_decay=config.training.weight_decay,
            betas=(0.9, 0.999)
        )
        
        # 7. Setup Learning Rate Scheduler
        self.scheduler = None  # Will be set after dataloader is created
        
        logger.info("[Trainer] Initialization complete")
    
    def _print_trainable_params(self):
        """Print the number of trainable parameters."""
        trainable = 0
        total = 0
        for _, param in self.model.named_parameters():
            total += param.numel()
            if param.requires_grad:
                trainable += param.numel()
        
        logger.info(
            f"[LoRA] Trainable: {trainable:,} / {total:,} "
            f"({100 * trainable / total:.2f}%)"
        )
    
    def train_step(self, batch: dict) -> float:
        """Single optimization step with gradient accumulation."""
        self.model.train()
        
        # Move batch to device
        batch = {k: v.to(self.device) for k, v in batch.items()}
        
        # Forward pass
        outputs = self.model(
            pixel_values=batch.get("pixel_values"),
            input_ids=batch["input_ids"],
            attention_mask=batch.get("attention_mask"),
            labels=batch["labels"]
        )
        
        loss = outputs.loss / self.config.training.gradient_accumulation_steps
        
        # Backward pass
        loss.backward()
        
        return loss.item() * self.config.training.gradient_accumulation_steps
    
    def train(self, dataloader):
        """Full training loop."""
        logger.info("=" * 60)
        logger.info("Starting Training")
        logger.info(f"  Epochs: {self.config.training.num_epochs}")
        logger.info(f"  Batch Size: {self.config.training.batch_size}")
        logger.info(f"  Gradient Accumulation: {self.config.training.gradient_accumulation_steps}")
        logger.info(f"  Effective Batch Size: {self.config.training.batch_size * self.config.training.gradient_accumulation_steps}")
        logger.info(f"  Learning Rate: {self.config.training.learning_rate}")
        logger.info("=" * 60)
        
        total_steps = len(dataloader) * self.config.training.num_epochs
        
        # Setup scheduler
        from transformers import get_linear_schedule_with_warmup
        warmup_steps = int(total_steps * self.config.training.warmup_ratio)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        for epoch in range(self.config.training.num_epochs):
            epoch_loss = 0.0
            epoch_steps = 0
            
            progress = tqdm(
                dataloader,
                desc=f"Epoch {epoch + 1}/{self.config.training.num_epochs}",
                leave=True
            )
            
            for step, batch in enumerate(progress):
                loss = self.train_step(batch)
                epoch_loss += loss
                epoch_steps += 1
                
                # Gradient accumulation step
                if (step + 1) % self.config.training.gradient_accumulation_steps == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config.training.max_grad_norm
                    )
                    
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    
                    self.global_step += 1
                    self.losses.append(loss)
                    
                    # Update progress bar
                    progress.set_postfix({
                        'loss': f'{loss:.4f}',
                        'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'
                    })
                    
                    # Logging
                    if self.global_step % self.config.training.logging_steps == 0:
                        avg_loss = sum(self.losses[-10:]) / min(10, len(self.losses))
                        logger.info(
                            f"Step {self.global_step} | Loss: {loss:.4f} | "
                            f"Avg Loss (10): {avg_loss:.4f} | "
                            f"LR: {self.scheduler.get_last_lr()[0]:.2e}"
                        )
                    
                    # Save checkpoint
                    if self.global_step % self.config.training.save_steps == 0:
                        self.save_checkpoint(f"checkpoint-{self.global_step}")
            
            # End of epoch
            avg_epoch_loss = epoch_loss / epoch_steps
            logger.info(f"Epoch {epoch + 1} Complete | Avg Loss: {avg_epoch_loss:.4f}")
            
            # Save best model
            if avg_epoch_loss < self.best_loss:
                self.best_loss = avg_epoch_loss
                self.save_checkpoint("best_model")
                logger.info(f"New best model saved! Loss: {self.best_loss:.4f}")
        
        # Final save
        self.save_checkpoint("final_model")
        self.plot_loss()
        logger.info("Training Complete!")
    
    def save_checkpoint(self, name: str):
        """Save model checkpoint."""
        save_path = os.path.join(self.config.paths.checkpoint_dir, name)
        self.model.save_pretrained(save_path)
        self.processor.save_pretrained(save_path)
        
        # Save training state
        torch.save({
            'global_step': self.global_step,
            'losses': self.losses,
            'optimizer_state': self.optimizer.state_dict(),
            'scheduler_state': self.scheduler.state_dict() if self.scheduler else None,
            'config': self.config
        }, os.path.join(save_path, "training_state.pt"))
        
        logger.info(f"[Checkpoint] Saved to {save_path}")
    
    def plot_loss(self):
        """Generate and save the training loss curve."""
        plt.figure(figsize=(12, 5))
        
        # Main loss curve
        plt.subplot(1, 2, 1)
        plt.plot(self.losses, label='Training Loss', color='#2563eb', alpha=0.7)
        
        # Add smoothed curve
        if len(self.losses) > 10:
            window = min(50, len(self.losses) // 5)
            smoothed = [
                sum(self.losses[max(0, i-window):i+1]) / min(i+1, window)
                for i in range(len(self.losses))
            ]
            plt.plot(smoothed, label=f'Smoothed (window={window})', color='#dc2626', linewidth=2)
        
        plt.title('Stage 1: Llama 3.2 Vision Fine-Tuning Loss', fontsize=12, fontweight='bold')
        plt.xlabel('Training Step')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Loss distribution
        plt.subplot(1, 2, 2)
        plt.hist(self.losses, bins=50, color='#2563eb', alpha=0.7, edgecolor='black')
        plt.axvline(x=sum(self.losses)/len(self.losses), color='#dc2626', linestyle='--', label=f'Mean: {sum(self.losses)/len(self.losses):.4f}')
        plt.title('Loss Distribution', fontsize=12, fontweight='bold')
        plt.xlabel('Loss')
        plt.ylabel('Frequency')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        save_path = os.path.join(self.config.paths.output_dir, 'phase1_loss_curve.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"[Plot] Loss curve saved to {save_path}")


def test_pipeline_shape(config: Phase1Config):
    """Verify tensor shapes before the big run (Deep-ML Style)."""
    logger.info("=" * 60)
    logger.info("Running Shape Verification Test")
    logger.info("=" * 60)
    
    # Simulate batch
    batch_size = config.training.batch_size
    seq_length = 512
    image_size = 560  # Llama 3.2 Vision default
    
    if torch.cuda.is_available():
        device = "cuda"
        dtype = torch.bfloat16
    else:
        device = "cpu"
        dtype = torch.float32
    
    # Create dummy tensors
    dummy_pixel_values = torch.randn(
        batch_size, 4, 3, image_size, image_size,  # 4 tiles for Llama 3.2 Vision
        device=device, dtype=dtype
    )
    dummy_input_ids = torch.randint(0, 32000, (batch_size, seq_length), device=device)
    dummy_attention_mask = torch.ones(batch_size, seq_length, device=device, dtype=torch.long)
    dummy_labels = dummy_input_ids.clone()
    
    logger.info(f"  pixel_values: {dummy_pixel_values.shape}")
    logger.info(f"  input_ids: {dummy_input_ids.shape}")
    logger.info(f"  attention_mask: {dummy_attention_mask.shape}")
    logger.info(f"  labels: {dummy_labels.shape}")
    logger.info("Shape Verification: PASSED âœ“")
    
    # Memory check
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        logger.info(f"  GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
    
    return True


def main():
    parser = argparse.ArgumentParser(description="Stage 1: The Captioner - LoRA Training")
    parser.add_argument("--test-only", action="store_true", help="Run shape verification only")
    parser.add_argument("--learning-rate", type=float, default=None)
    parser.add_argument("--epochs", type=int, default=None)
    parser.add_argument("--batch-size", type=int, default=None)
    args = parser.parse_args()
    
    # Build config with overrides
    overrides = {}
    if args.learning_rate:
        overrides["learning_rate"] = args.learning_rate
    if args.epochs:
        overrides["num_epochs"] = args.epochs
    if args.batch_size:
        overrides["batch_size"] = args.batch_size
    
    config = get_config(**overrides)
    
    # Test mode
    if args.test_only:
        test_pipeline_shape(config)
        return
    
    # Full training
    logger.info(f"[Config] Output: {config.paths.output_dir}")
    logger.info(f"[Config] Dataset: {config.paths.dataset_dir}")
    
    trainer = MultimodalTrainer(config)
    dataloader = create_dataloader(config, trainer.processor)
    
    logger.info(f"[Data] Loaded {len(dataloader.dataset)} samples")
    logger.info(f"[Data] {len(dataloader)} batches per epoch")
    
    trainer.train(dataloader)


if __name__ == "__main__":
    main()
